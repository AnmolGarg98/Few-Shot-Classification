{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MatchingNET_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SdWLAJRhvYj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.0.0\n",
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "nkMGI9BCrzyJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "615bf5ec-492b-42f2-b50a-db2d324a9067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.0.0\n",
            "  Downloading Keras-2.0.0.tar.gz (191 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 191 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.0.0) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from keras==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.44.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (13.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (4.2.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.7.1-cp37-cp37m-manylinux2010_x86_64.whl (495.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 495.0 MB 30 kB/s \n",
            "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 28 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.6.3-cp37-cp37m-manylinux2010_x86_64.whl (463.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 463.8 MB 34 kB/s \n",
            "\u001b[?25hCollecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 33.7 MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow\n",
            "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 13 kB/s \n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.6.1-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 14 kB/s \n",
            "\u001b[?25h  Downloading tensorflow-2.6.0-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (2.8.0)\n",
            "  Downloading tensorflow-2.5.3-cp37-cp37m-manylinux2010_x86_64.whl (460.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.3 MB 9.7 kB/s \n",
            "\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 36.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras==2.0.0) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras==2.0.0) (3.2.0)\n",
            "Building wheels for collected packages: keras, wrapt\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-2.0.0-py3-none-any.whl size=227982 sha256=04884b3fd301f1bc516c89a00140f0c0b9b60d85f7f128e47597dab222423dbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/1a/f7/d34a44e2fa0c03377e24bae68eacaeb12ea66fb3d297612612\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68724 sha256=646f779694c4e11118c0b722013b837e0de5eab9ce3bed807d28482887d1457e\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built keras wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, keras-nightly, gast, flatbuffers, tensorflow, keras\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.44.0\n",
            "    Uninstalling grpcio-1.44.0:\n",
            "      Successfully uninstalled grpcio-1.44.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.0\n",
            "    Uninstalling wrapt-1.14.0:\n",
            "      Successfully uninstalled wrapt-1.14.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.4.0 grpcio-1.34.1 keras-2.0.0 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 tensorflow-2.5.3 tensorflow-estimator-2.5.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 26 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 28.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.34.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=534a5ab2252383944722adaf4d2d2067943a0548407be08580aea0564bd37eae\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.3\n",
            "    Uninstalling tensorflow-2.5.3:\n",
            "      Successfully uninstalled tensorflow-2.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Lambda\n",
        "from keras.layers.merge import Maximum\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "from keras.layers.merge import _Merge\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "iKKCraVIsDiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2eb0dc5-8a60-4845-b5dc-c59c32321196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, Input, Lambda\n",
        "from keras.layers.merge import Maximum\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf \n",
        "from keras.layers.merge import _Merge\n"
      ],
      "metadata": {
        "id": "vzutC7aUn18s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MatchCosine(_Merge):\n",
        "    \"\"\"\n",
        "        Matching network with cosine similarity metric\n",
        "    \"\"\"\n",
        "    def __init__(self,nway=5,**kwargs):\n",
        "        super(MatchCosine,self).__init__(**kwargs)\n",
        "        self.eps = 1e-10\n",
        "        self.nway = nway\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, list) or len(input_shape) != self.nway+2:\n",
        "            raise ValueError('A ModelCosine layer should be called on a list of inputs of length %d'%(self.nway+2))\n",
        "\n",
        "    def call(self,inputs):\n",
        "        \"\"\"\n",
        "        inputs in as array which contains the support set the embeddings, \n",
        "        the target embedding as the second last value in the array, and true class of target embedding as the last value in the array\n",
        "        \"\"\" \n",
        "        similarities = []\n",
        "\n",
        "        targetembedding = inputs[-2] # embedding of the query image\n",
        "        numsupportset = len(inputs)-2\n",
        "        for ii in range(numsupportset):\n",
        "            supportembedding = inputs[ii] # embedding for i^{th} member in the support set\n",
        "\n",
        "            sum_support = tf.reduce_sum(tf.square(supportembedding), 1, keep_dims=True)\n",
        "            supportmagnitude = tf.rsqrt(tf.clip_by_value(sum_support, self.eps, float(\"inf\"))) #reciprocal of the magnitude of the member of the support \n",
        "\n",
        "            sum_query = tf.reduce_sum(tf.square(targetembedding), 1, keep_dims=True)\n",
        "            querymagnitude = tf.rsqrt(tf.clip_by_value(sum_query, self.eps, float(\"inf\"))) #reciprocal of the magnitude of the query image\n",
        "\n",
        "            dot_product = tf.matmul(tf.expand_dims(targetembedding,1),tf.expand_dims(supportembedding,2))\n",
        "            dot_product = tf.squeeze(dot_product,[1])\n",
        "\n",
        "            cosine_similarity = dot_product*supportmagnitude*querymagnitude\n",
        "            similarities.append(cosine_similarity)\n",
        "\n",
        "        similarities = tf.concat(axis=1,values=similarities)\n",
        "        softmax_similarities = tf.nn.softmax(similarities)\n",
        "        preds = tf.squeeze(tf.matmul(tf.expand_dims(softmax_similarities,1),inputs[-1]))\n",
        "        \n",
        "        preds.set_shape((inputs[0].shape[0],self.nway))\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        input_shapes = input_shape\n",
        "        return (input_shapes[0][0],self.nway)\n",
        "\n",
        "# Bonus: Matching network with Euclidean metrtic\n",
        "class MatchEuclidean(_Merge):\n",
        "    \"\"\"\n",
        "        Matching network with Euclidean metric\n",
        "    \"\"\"\n",
        "    def __init__(self,nway=5,**kwargs):\n",
        "        super(MatchEuclidean,self).__init__(**kwargs)\n",
        "        self.eps = 1e-10\n",
        "        self.nway = nway\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, list) or len(input_shape) != self.nway+2:\n",
        "            raise ValueError('A ModelEuclidean layer should be called on a list of inputs of length %d'%(self.nway+2))\n",
        "\n",
        "    def call(self,inputs):\n",
        "        \"\"\"\n",
        "        inputs in as array which contains the support set the embeddings, the target embedding as the second last value in the array, and true class of target embedding as the last value in the array\n",
        "        \"\"\" \n",
        "        similarities = []\n",
        "\n",
        "        targetembedding = inputs[-2]\n",
        "        numsupportset = len(inputs)-2\n",
        "        for ii in range(numsupportset):\n",
        "            supportembedding = inputs[ii]\n",
        "            dd = tf.negative(tf.sqrt(tf.reduce_sum(tf.square(supportembedding-targetembedding),1,keep_dims=True)))\n",
        "\n",
        "            similarities.append(dd)\n",
        "\n",
        "        similarities = tf.concat(axis=1,values=similarities)\n",
        "        softmax_similarities = tf.nn.softmax(similarities)\n",
        "        preds = tf.squeeze(tf.matmul(tf.expand_dims(softmax_similarities,1),inputs[-1]))\n",
        "        \n",
        "        preds.set_shape((inputs[0].shape[0],self.nway))\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        input_shapes = input_shape\n",
        "        return (input_shapes[0][0],self.nway)\n",
        "\n",
        "# Siamese network like interaction\n",
        "class Siamify(_Merge):\n",
        "    def _merge_function(self,inputs):\n",
        "        return tf.negative(tf.abs(inputs[0]-inputs[1]))"
      ],
      "metadata": {
        "id": "p0lj5y3goJW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2191)  # for reproducibility\n",
        "\n",
        "class OmniglotNShotDataset():\n",
        "    def __init__(self,batch_size,classes_per_set=5,samples_per_class=1,trainsize=32000,valsize=10000):\n",
        "\n",
        "        \"\"\"\n",
        "        Constructs an N-Shot omniglot Dataset\n",
        "        :param batch_size: Experiment batch_size\n",
        "        :param classes_per_set: Integer indicating the number of classes per set\n",
        "        :param samples_per_class: Integer indicating samples per class\n",
        "        e.g. For a 20-way, 1-shot learning task, use classes_per_set=20 and samples_per_class=1\n",
        "             For a 5-way, 10-shot learning task, use classes_per_set=5 and samples_per_class=10\n",
        "        \"\"\"\n",
        "        self.x = np.load(\"/content/drive/MyDrive/Data_colab/data.npy\")\n",
        "        self.x = np.reshape(self.x, [-1, 20, 28, 28, 1])\n",
        "        shuffle_classes = np.arange(self.x.shape[0])\n",
        "        np.random.shuffle(shuffle_classes)\n",
        "        self.x = self.x[shuffle_classes]\n",
        "        self.x_train, self.x_val  = self.x[:1200], self.x[1200:]\n",
        "        self.normalization()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_classes = self.x.shape[0]\n",
        "        self.classes_per_set = classes_per_set\n",
        "        self.samples_per_class = samples_per_class\n",
        "\n",
        "        self.indexes = {\"train\": 0, \"val\": 0}\n",
        "        self.datasets = {\"train\": self.x_train, \"val\": self.x_val}\n",
        "        self.datasets_cache = {\"train\": self.packslice(self.datasets[\"train\"],trainsize),\n",
        "                               \"val\": self.packslice(self.datasets[\"val\"],valsize)}\n",
        "\n",
        "    def normalization(self):\n",
        "        \"\"\"\n",
        "        Normalizes our data, to have a mean of 0 and sd of 1\n",
        "        \"\"\"\n",
        "        self.mean = np.mean(self.x_train)\n",
        "        self.std = np.std(self.x_train)\n",
        "        self.max = np.max(self.x_train)\n",
        "        self.min = np.min(self.x_train)\n",
        "        print(\"train_shape\", self.x_train.shape, \"val_shape\", self.x_val.shape)\n",
        "        print(\"before_normalization\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
        "        self.x_train = (self.x_train - self.mean) / self.std\n",
        "        self.x_val = (self.x_val - self.mean) / self.std\n",
        "        self.mean = np.mean(self.x_train)\n",
        "        self.std = np.std(self.x_train)\n",
        "        self.max = np.max(self.x_train)\n",
        "        self.min = np.min(self.x_train)\n",
        "        print(\"after_normalization\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
        "        \n",
        "    def packslice(self, data_pack, numsamples):\n",
        "        \"\"\"\n",
        "        Collects 1000 batches data for N-shot learning\n",
        "        :param data_pack: Data pack to use (any one of train, val, test)\n",
        "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
        "        \"\"\"\n",
        "        n_samples = self.samples_per_class * self.classes_per_set\n",
        "        support_cacheX = []\n",
        "        support_cacheY = []\n",
        "        target_cacheY = []\n",
        "        \n",
        "        for iiii in range(numsamples):\n",
        "            slice_x = np.zeros((n_samples+1,28,28,1))\n",
        "            slice_y = np.zeros((n_samples,))\n",
        "            \n",
        "            ind = 0\n",
        "            pinds = np.random.permutation(n_samples)\n",
        "            classes = np.random.choice(data_pack.shape[0],self.classes_per_set,False) # chosen classes\n",
        "            \n",
        "            x_hat_class = np.random.randint(self.classes_per_set) # target class\n",
        "            \n",
        "            for j, cur_class in enumerate(classes):  # each class\n",
        "                example_inds = np.random.choice(data_pack.shape[1],self.samples_per_class,False)\n",
        "                \n",
        "                for eind in example_inds:\n",
        "                    slice_x[pinds[ind],:,:,:] = data_pack[cur_class][eind]\n",
        "                    slice_y[pinds[ind]] = j\n",
        "                    ind += 1\n",
        "                \n",
        "                if j == x_hat_class:\n",
        "                    slice_x[n_samples,:,:,:] = data_pack[cur_class][np.random.choice(data_pack.shape[1])]\n",
        "                    target_y = j\n",
        "\n",
        "            support_cacheX.append(slice_x)\n",
        "            support_cacheY.append(keras.utils.to_categorical(slice_y,self.classes_per_set))\n",
        "            target_cacheY.append(keras.utils.to_categorical(target_y,self.classes_per_set)[0])\n",
        "            \n",
        "        return np.array(support_cacheX), np.array(support_cacheY), np.array(target_cacheY)"
      ],
      "metadata": {
        "id": "vM_tjrxfnlmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qGt2FBIgnYaY",
        "outputId": "495200d3-b5ff-4cff-f555-62d96da33367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_shape (1200, 20, 28, 28, 1) val_shape (422, 20, 28, 28, 1)\n",
            "before_normalization mean 0.9189295 max 1.0 min 0.0 std 0.21971507\n",
            "after_normalization mean -1.9403703e-06 max 0.36898008 min -4.182369 std 0.99999905\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:47: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:349: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3014: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-4-e16161696b98>:75: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:675: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2554: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:766: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:762: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 64000 samples, validate on 20000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:140: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:145: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:150: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:298: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:306: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "64000/64000 [==============================] - 276s - loss: 1.1340 - acc: 0.5514 - val_loss: 0.7229 - val_acc: 0.7244\n",
            "Epoch 2/10\n",
            "64000/64000 [==============================] - 263s - loss: 0.6193 - acc: 0.7646 - val_loss: 0.3859 - val_acc: 0.8572\n",
            "Epoch 3/10\n",
            "64000/64000 [==============================] - 263s - loss: 0.4294 - acc: 0.8408 - val_loss: 0.3003 - val_acc: 0.8889\n",
            "Epoch 4/10\n",
            "64000/64000 [==============================] - 263s - loss: 0.3314 - acc: 0.8786 - val_loss: 0.2623 - val_acc: 0.9054\n",
            "Epoch 5/10\n",
            "64000/64000 [==============================] - 260s - loss: 0.2778 - acc: 0.8982 - val_loss: 0.2265 - val_acc: 0.9188\n",
            "Epoch 6/10\n",
            "64000/64000 [==============================] - 260s - loss: 0.2311 - acc: 0.9174 - val_loss: 0.2037 - val_acc: 0.9283\n",
            "Epoch 7/10\n",
            "64000/64000 [==============================] - 258s - loss: 0.1982 - acc: 0.9294 - val_loss: 0.1771 - val_acc: 0.9372\n",
            "Epoch 8/10\n",
            "64000/64000 [==============================] - 258s - loss: 0.1765 - acc: 0.9378 - val_loss: 0.1694 - val_acc: 0.9428\n",
            "Epoch 9/10\n",
            "48832/64000 [=====================>........] - ETA: 55s - loss: 0.1563 - acc: 0.9444"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e379b114da9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m model.fit([data.datasets_cache[\"train\"][0],data.datasets_cache[\"train\"][1]],data.datasets_cache[\"train\"][2],\n\u001b[1;32m    106\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m           epochs=10,batch_size=32,verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2075\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "bsize = 32 # batch size\n",
        "classes_per_set = 5 # classes per set or 5-way\n",
        "samples_per_class = 1 # samples per class 1-shot\n",
        "\n",
        "data = OmniglotNShotDataset(batch_size=bsize,classes_per_set=classes_per_set,samples_per_class=samples_per_class,trainsize=64000,valsize=20000)\n",
        "\n",
        "# Image embedding using Deep Convolutional Network\n",
        "conv1 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
        "bnorm1 = BatchNormalization()\n",
        "mpool1 = MaxPooling2D((2,2),padding='same')\n",
        "conv2 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
        "bnorm2 = BatchNormalization()\n",
        "mpool2 = MaxPooling2D((2,2),padding='same')\n",
        "conv3 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
        "bnorm3 = BatchNormalization()\n",
        "mpool3 = MaxPooling2D((2,2),padding='same')\n",
        "conv4 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
        "bnorm4 = BatchNormalization()\n",
        "mpool4 = MaxPooling2D((2,2),padding='same')\n",
        "fltn = Flatten()\n",
        "\n",
        "# Function that generarates Deep CNN embedding given the input image x\n",
        "def convembedding(x):\n",
        "    x = conv1(x)\n",
        "    x = bnorm1(x)\n",
        "    x = mpool1(x)\n",
        "    x = conv2(x)\n",
        "    x = bnorm2(x)\n",
        "    x = mpool2(x)\n",
        "    x = conv3(x)\n",
        "    x = bnorm3(x)\n",
        "    x = mpool3(x)\n",
        "    x = conv4(x)\n",
        "    x = bnorm4(x)\n",
        "    x = mpool4(x)\n",
        "    x = fltn(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "# Relational embedding comprising a 4 layer MLP\n",
        "d1 = Dense(64,activation='relu')\n",
        "dbnrm1 = BatchNormalization()\n",
        "d2 = Dense(64,activation='relu')\n",
        "dbnrm2 = BatchNormalization()\n",
        "d3 = Dense(64,activation='relu')\n",
        "dbnrm3 = BatchNormalization()\n",
        "d4 = Dense(64,activation='relu')\n",
        "dbnrm4 = BatchNormalization()\n",
        "\n",
        "def relationalembedding(x):\n",
        "    x = d1(x)\n",
        "    x = dbnrm1(x)\n",
        "    x = d2(x)\n",
        "    x = dbnrm2(x)\n",
        "    x = d3(x)\n",
        "    x = dbnrm3(x)\n",
        "    x = d4(x)\n",
        "    x = dbnrm4(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "numsupportset = samples_per_class*classes_per_set\n",
        "input1 = Input((numsupportset+1,28,28,1))\n",
        "\n",
        "# CNN embedding support set and query image\n",
        "convolutionlayers = []\n",
        "for lidx in range(numsupportset):\n",
        "    convolutionlayers.append(convembedding(Lambda(lambda x: x[:,lidx,:,:,:])(input1)))\n",
        "targetembedding = convembedding(Lambda(lambda x: x[:,-1,:,:,:])(input1))\n",
        "\n",
        "# Siamese like pairwise interactions\n",
        "siam = Siamify()\n",
        "pairwiseinteractions = defaultdict(list)\n",
        "\n",
        "# Get all pairwise Siamese interactions in the support set and generate of list of interactions\n",
        "# for each member of support set \n",
        "for tt in combinations(range(numsupportset),2):\n",
        "    aa = siam([convolutionlayers[tt[0]],convolutionlayers[tt[1]]])\n",
        "    pairwiseinteractions[tt[0]].append(aa)\n",
        "    pairwiseinteractions[tt[1]].append(aa)\n",
        "\n",
        "# Get Siamese interactions for query image\n",
        "targetinteractions = []\n",
        "for i in range(numsupportset):\n",
        "    aa = siam([targetembedding,convolutionlayers[i]])\n",
        "    targetinteractions.append(aa)  \n",
        "    pairwiseinteractions[i].append(aa) # add this interaction to the set of interaction for this member\n",
        "\n",
        "# Take 4 layer MLP transform on Max pooling of interactions to serve as Full Context Embedding (FCE)\n",
        "maxi = Maximum()\n",
        "modelinputs = []\n",
        "for i in range(numsupportset):\n",
        "    modelinputs.append(relationalembedding(maxi(pairwiseinteractions[i])))\n",
        "modelinputs.append(relationalembedding(maxi(targetinteractions)))\n",
        "\n",
        "supportlabels = Input((numsupportset,classes_per_set))\n",
        "modelinputs.append(supportlabels)\n",
        "\n",
        "knnsimilarity = MatchEuclidean(nway=classes_per_set)(modelinputs)\n",
        "\n",
        "model = Model(inputs=[input1,supportlabels],outputs=knnsimilarity)\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.fit([data.datasets_cache[\"train\"][0],data.datasets_cache[\"train\"][1]],data.datasets_cache[\"train\"][2],\n",
        "          validation_data=[[data.datasets_cache[\"val\"][0],data.datasets_cache[\"val\"][1]],data.datasets_cache[\"val\"][2]],\n",
        "          epochs=10,batch_size=32,verbose=1)"
      ]
    }
  ]
}